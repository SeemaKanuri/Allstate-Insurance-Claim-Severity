---
title: "2nd Model with h2o Gradient Boosting Machine (GBM)"
author: "Seema Rani Kanuri"
date: "December 10, 2016"
output: html_document
---
#Model : 2

## Introduction : Allstate Claims Severity
We aspire to demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate’s efforts to ensure a worry-free customer experience.

### Goal:
The Goal is to predict the loss based on the severity of the claim using theh2o Gradient Boosting Machine (GBM) Learning method. 

### Task:
We have to predict the cost and the severity claim of the Allstate, a personal insurer in the United States.


```{r setup, warning=F, results='hide'}
set.seed(0) #  setting a seed will ensure reproducible results (not R's seed)


train<-read.csv('F:/OneDrive - Texas Tech University/MastersDocuments/DS- Multivariate Analysis/Allstate/train.csv')
test<-read.csv('F:/OneDrive - Texas Tech University/MastersDocuments/DS- Multivariate Analysis/Allstate/test.csv')
```

## Introduction to the Data
Each row in this dataset represents an insurance claim. You must predict the value for the ‘loss’ column. Variables prefaced with ‘cat’ are categorical, while those prefaced with ‘cont’ are continuous.(source:https://www.kaggle.com/c/allstate-claims-severity)

### File descriptions:
There are no missing values in the dataset.

train.csv - the training set
test.csv - the test set. You must predict the loss value for the ids in this file.
sample_submission.csv - a sample submission file in the correct format


```{r h2o-cluster, warning=F, echo=FALSE}
#str(train)
#str(test)
head(train[1:10])
sum(is.na(train)) 
sum(is.na(test))
```

### The data & Model
Lets look at 25 rows:

id
116 categorical features
14 continuous features
Loss (label to predict)

```{r}
train<-train[,-1]
test_label<-test[,1]
test<-test[,-1]
```

## Initialization

First, we will create three splits for train/test/valid independent data sets.We will train a data set on one set and use the others to test the validity of model by ensuring that it can predict accurately on data the model has not been shown.

```{r}
index_df<-sample(1:(dim(train)[1]), 0.2*dim(train)[1], replace=FALSE)
train_index<-train[-index_df,]
valid_index<-train[index_df,]
```

The second set will be used for validation most of the time. The third set will be withheld until the end, to ensure that our validation accuracy is consistent with data we have never seen during the iterative process. 

```{r}
train_index[,ncol(train_index)]<-log(train_index[,ncol(train_index)])
valid_index[,ncol(train_index)]<-log(valid_index[,ncol(valid_index)])
```

## Setting Up and Connecting to a H2O Cluster
Let’s first load some packages

```{r}
# H2O is an R package
library(h2o)

# Create an H2O cloud 
h2oPackage<-h2o.init(
  nthreads=-1,            #use available threads
  max_mem_size = "16G")   # specify the memory size for the H2O cloud

h2o.removeAll() ## clean slate - just in case the cluster was already running

```


Assign the first result the R variable train and the H2O name train.hex

```{r  hide}
train_index.hex<-as.h2o(train_index)
valid_index.hex<-as.h2o(valid_index)  # R valid, H2O valid.hex
test.hex<-as.h2o(test)
```


### Run our predictive model, Training a h2o Gradient Boosting Machine (GBM) Model
Using GBM in h2o for prediction First we will use all default settings, and then make some changes,where the parameters and defaults are described.(source:http://blog.h2o.ai/2016/06/h2o-gbm-tuning-tutorial-for-r/)

```{r , warning=F, results='hide' }

h2oGBM_Model<-h2o.gbm(
                   x=1:(ncol(train_index.hex)-1),                  ## the predictor columns, by column index_df
                   y=ncol(train_index.hex),                        ## the target index_df (what we are predicting)
  	               training_frame=train_index.hex,                 ## the H2O frame for training
                   validation_frame=valid_index.hex,               ## the H2O frame for validation (not required)
                   ntrees = 1000,                                  ## decrease the trees, mostly to allow for run time
                   learn_rate = 0.3,                               ## increase the learning rate (from 0.3)
                   max_depth = 100,                                 ## increase the depth (from 5)
                   sample_rate = 0.7,                              ## use a random 70% of the rows to fit each tree
                   col_sample_rate = 0.7,                          ## use 70% of the columns to fit each tree
                   stopping_rounds = 2,                            ## 
                   stopping_tolerance = 0.01,                      ##
                   score_each_iteration = T,                       ##
                   seed = 2000000                                  ## Set the random seed for reproducability
                  )

```


##  View information about the Gradient Boosting Machine model.
Keys to look for are validation performance and variable importance

```{r}
summary(h2oGBM_Model)                                                 ## View information about the model.
```

## Using the model for prediction

```{r}
Prediction<-(as.matrix(predict(h2oGBM_Model, test.hex)))
Predicted_values<-exp((Prediction)/2)
```
## Generating the predicted values

```{r}
Predicted_df = as.data.frame(Predicted_values)
Predicted_df = data.frame(ImageId = seq(1,length(Predicted_df$predict)), Label = Predicted_df$predict)
write.csv(Predicted_df, file = "F:/OneDrive - Texas Tech University/MastersDocuments/DS- Multivariate Analysis/Allstate/2-GBM-h2o.csv", row.names=F)

```

### All done, shutdown H2O    

```{r}
h2o.shutdown(prompt=FALSE)
```
## Conclusion


I tried of using Python but end up struggling a long time installing few packages like `TensorFlow` and ` Keras` and I spend vertually a long time on setting the framework for the required packages.So like last time, again I end up doing my project using R.

However R seems to be an easy choice where I was able to do the analysis in a quick time. To train the data I have used 3 hidden layers Deep Learning algorithms with each of 1280 nodes and an epoch of 1000 using the `h2o` package on a subset lof data which lasted for longer than 80 minutes. 

Apart from   3 hidden layers Deep Learning algorithms  using the `h2o` package, I alos tried `h20 Gradient Boosting Machine(GBM) algorithms` and ` h2o.randomForest algorithms`. Using ``h20 Gradient Boosting Machine algorithms`the  best accuracy I got is with leadership Board score of 1158.8236.

However the best accuracy I got is with 3 hidden layers Deep Learning algorithms with each of 1280 nodes with leadership Board score of 1114.3580.

I have produced 3 different output files for the loss values to show how predicting a loss value correctly can enchance overall claims experience for the customer as well as the Insurance company. These output files are produced using 3 different models  :-

### 3_deeplearning-h2o.csv (produced by using the h20 Deep Learning algorithms)
### 2_GBM-h2o.csv (produced by using the h20 GBM algorithms)
### 1_RF-h2o.csv (produced by using the h20 Random Forest algorithms)

## Resources

[Gradient Boosting Machine](https://www.rdocumentation.org/packages/h2o/versions/3.10.0.8/topics/h2o.gbm)
[H2O GBM Tuning Tutorial for R](http://blog.h2o.ai/2016/06/h2o-gbm-tuning-tutorial-for-r/)
[h2o-tutorials](https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/gbm-randomforest/GBM_RandomForest_Example.R)
[Running Models](http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/rtutorial.html)